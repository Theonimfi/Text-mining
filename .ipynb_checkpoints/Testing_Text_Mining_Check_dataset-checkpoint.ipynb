{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Theonimfi/Text-mining/blob/main/Testing_Text_Mining_Check_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "58rijbWU8BBh"
   },
   "outputs": [],
   "source": [
    "# !tar -xvf  '/content/drive/MyDrive/Data Science and AI/Text mining/enwiki20220701-stripped.tgz' -C '/content/drive/MyDrive/Data Science and AI/Text mining/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lclvC4iirb4K"
   },
   "outputs": [],
   "source": [
    "# Do not uncomment the below \n",
    "\n",
    "# ! pip install pynvml\n",
    "# ! git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "# ! python rapidsai-csp-utils/colab/env-check.py\n",
    "# ! bash rapidsai-csp-utils/colab/update_gcc.sh\n",
    "# import os\n",
    "# os._exit(00)\n",
    "# import condacolab\n",
    "# condacolab.install()\n",
    "# import condacolab\n",
    "# condacolab.check()\n",
    "#!python rapidsai-csp-utils/colab/install_rapids.py stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8YGu3NoT6f6h"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get the data from My drive\n",
    "file1 = open('/content/drive/MyDrive/Data Science and AI/Text mining_Shared/enwiki20220701-stripped/AB/wiki_63', 'r')\n",
    "Lines = file1.readlines()\n",
    "  \n",
    "content = []\n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    content.append(json.loads(line.split('\\n')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aed_v9VaZGus"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Add data to a pandas dataframe\n",
    "df = pd.DataFrame(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoSkN5G5awFn",
    "outputId": "f414c619-112e-460a-8d68-6ae33f2e08de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98939"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['text'].values == '').sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tirA2DGedjoT"
   },
   "source": [
    "Play with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiyirBGJbM_0",
    "outputId": "06a329d3-25e5-440f-ab02-6c230c626542"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and download example data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VaCfj4gAbskV"
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('of')\n",
    "\n",
    "#defining the function to remove punctuation except dot (.)\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation or i == '.'])\n",
    "    return punctuationfree\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = re.split('W+',text)\n",
    "    return tokens\n",
    "\n",
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrVFIFAFrFMb",
    "outputId": "84d3fbeb-e4b1-48cf-c60b-5187f139607c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Keep only rows with non empty text\n",
    "df_new = df[df.text != '']\n",
    "df_new['text_preprocessed'] = df_new['text']\n",
    "\n",
    "# Reindexing\n",
    "df_new.index = range(len(df_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3L7TEukci38",
    "outputId": "7d31829a-678e-451c-8006-dd1a7954f88c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of text\n",
    "df_new['text_preprocessed']= df_new['text_preprocessed'].apply(lambda x: remove_punctuation(x))\n",
    "nltk.download('punkt')\n",
    "df_new['text_preprocessed']= df_new['text_preprocessed'].apply(lambda x: word_tokenize(str(x)))\n",
    "df_new['text_preprocessed']= df_new['text_preprocessed'].apply(lambda x: remove_stopwords(x))\n",
    "df_new['text_preprocessed'] = [' '.join(map(str, l)) for l in df_new['text_preprocessed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "m9K7HpN1LHSG",
    "outputId": "f8aad589-8d44-4fb3-c150-236695adb6aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Clepardia Kraków KS Clepardia Kraków Polish football club based Prądnik Biały district Kraków . They currently play IV Liga fifth tier Polish football league . History . The Krowodrza – Modrzejówka Sports Society founded 1909 one earliest clubs Krakow . Initially ground located Lubelska Street next military hospital later premises current military unit Wrocławska Street . The Prądnicki Sports Club established 1930 KS Prądnik . Its operation interrupted World War II reactivated 1955 . Since 1956 clubs football ground Prądnicka Street . In 1967 merger two clubs InterFactory District Sports Club Clepardia established . Over years club sport sections football handball judo . Currently football sections age levels judo functioning . Squad . Trener'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.index = range(len(df_new))\n",
    "df_new.text_preprocessed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8XAla361817",
    "outputId": "7648d687-0388-41f1-c02d-7c9701aa6765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are in  10000 out of  51268\n",
      "Took: 0:04:32.690307 time\n",
      "We are in  20000 out of  51268\n",
      "Took: 0:09:00.313763 time\n",
      "We are in  30000 out of  51268\n",
      "Took: 0:13:07.922627 time\n",
      "We are in  40000 out of  51268\n",
      "Took: 0:17:09.447511 time\n",
      "We are in  50000 out of  51268\n",
      "Took: 0:21:19.028386 time\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from itertools import *\n",
    "import datetime\n",
    "\n",
    "# Calculate time spend\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "spacy.require_gpu()\n",
    "counter = 0\n",
    "keep_docs = []\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Save in a list all the documents that contain more than one person\n",
    "for textt in df_new.text_preprocessed:\n",
    "  counter +=1\n",
    "  if counter % 10000 == 0:\n",
    "    end = datetime.datetime.now()\n",
    "    print(\"We are in \", counter, \"out of \", len(df_new.text_preprocessed))\n",
    "    print(\"Took:\", (end-start), \"time\")\n",
    "  doc = nlp(textt)\n",
    "  people = []\n",
    "  # Get the people entities included in a doc \n",
    "  for ent in doc.ents:\n",
    "    if ((ent.label_ == 'PERSON') and (ent.text not in people)):\n",
    "      people.append(ent.text)\n",
    "  if len(people) > 1:\n",
    "    keep_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCMgUl2aJQGJ",
    "outputId": "7757c9e1-e96e-4a1d-c2b0-748ab07a8fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31369\n"
     ]
    }
   ],
   "source": [
    "print(len(keep_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VY1KsRRnaSZX"
   },
   "outputs": [],
   "source": [
    "# Old way, takes a lot of time\n",
    "# import spacy\n",
    "# from spacy import displacy\n",
    "# from itertools import *\n",
    "# import datetime\n",
    "\n",
    "# start = datetime.datetime.now()\n",
    "\n",
    "# # Join text per 2000 document\n",
    "# texts = []\n",
    "# for i in range(0, len(df_new),2000):\n",
    "# \ttexts.append(\" \".join(text for text in df_new.text_preprocessed[i:i+2000]))\n",
    " \n",
    "# spacy.require_gpu()\n",
    "# total_entities = []\n",
    "# counter = 0\n",
    "# # keep_sentences = []\n",
    "# keep_docs = []\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# nlp.max_length = 100000000\n",
    "# people = []\n",
    "# for textt in df_new.text_preprocessed:\n",
    "#   counter +=1\n",
    "#   if counter % 10000 == 0:\n",
    "#     end = datetime.datetime.now()\n",
    "#     print(\"We are in \", counter, \"out of \", len(df_new.text_preprocessed))\n",
    "#     print(\"Took:\", (end-start), \"time\")\n",
    "#   doc = nlp(textt)\n",
    "#   for sent in doc.sents:\n",
    "#     count_people = 0\n",
    "#     sentence_entities = []\n",
    "#     # Extract all entity labels from a given doc and add them in key value pairs ie \"PERSON\": [Barack Obama, Michelle Obama]\n",
    "#     entities = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(sent.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
    "#     if 'PERSON' in entities:\n",
    "#       if len(entities['PERSON']) > 1:\n",
    "#         keep_sentences.append(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sed-TcfOActm",
    "outputId": "7b833661-c373-42b1-b5f4-fadf8916f664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are in  1 out of 1\n"
     ]
    }
   ],
   "source": [
    "# # Second option, if we save all the doc_entities in a list (takes also a lot of time)\n",
    "# keep_sentences = []\n",
    "# total_count = 0\n",
    "# for entity in total_entities:\n",
    "#   total_count += 1\n",
    "#   print(\"We are in \",total_count, \"out of\", len(total_entities)) \n",
    "#   for sentence in entity['sentences']:\n",
    "#     people_in_sentence = sum(person in str(sentence) for person in entity['PERSON'])\n",
    "#     if people_in_sentence > 1:\n",
    "#       keep_sentences.append(str(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCtpYEMvtVs9"
   },
   "outputs": [],
   "source": [
    "for doc in keep_docs:\n",
    "  for sent in doc.sents:\n",
    "    test = str(sent)\n",
    "    break;\n",
    "  break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3R8jCUjnRn-"
   },
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(test)), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14JE7wfpq0XE"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsYN2U8ElW7E"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large',device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9uVl97vq2MR"
   },
   "outputs": [],
   "source": [
    "print(keep_sentences[152])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RswrX8dFleBx"
   },
   "outputs": [],
   "source": [
    "sentences = keep_sentences\n",
    "# Function to parse the generated text and extract the triplets\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "for sentence in sentences:  \n",
    "\n",
    "  sentence = str(sentence)\n",
    "  # We need to use the tokenizer manually since we need special tokens.\n",
    "  extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(sentence,\n",
    "                                                                              return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "  extracted_triplets = extract_triplets(extracted_text[0])\n",
    "  print(extracted_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tRUxd1QgAaj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNEkrlM7yNpYlnWEExbSTh2",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1hPIrL_qcriA4e4S92Env6Ft70AVXH5uN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
