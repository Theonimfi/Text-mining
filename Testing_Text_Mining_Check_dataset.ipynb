{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Theonimfi/Text-mining/blob/main/Testing_Text_Mining_Check_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58rijbWU8BBh"
      },
      "outputs": [],
      "source": [
        "# !tar -xvf  '/content/drive/MyDrive/Data Science and AI/Text mining/enwiki20220701-stripped.tgz' -C '/content/drive/MyDrive/Data Science and AI/Text mining/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install pynvml\n",
        "# ! git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "# ! python rapidsai-csp-utils/colab/env-check.py\n",
        "# ! bash rapidsai-csp-utils/colab/update_gcc.sh\n",
        "# import os\n",
        "# os._exit(00)\n",
        "# import condacolab\n",
        "# condacolab.install()\n",
        "# import condacolab\n",
        "# condacolab.check()\n",
        "#!python rapidsai-csp-utils/colab/install_rapids.py stable"
      ],
      "metadata": {
        "id": "lclvC4iirb4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YGu3NoT6f6h"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Get the data from My drive\n",
        "file1 = open('/content/drive/MyDrive/Data Science and AI/Text mining_Shared/enwiki20220701-stripped/AB/wiki_63', 'r')\n",
        "Lines = file1.readlines()\n",
        "  \n",
        "content = []\n",
        "count = 0\n",
        "# Strips the newline character\n",
        "for line in Lines:\n",
        "    content.append(json.loads(line.split('\\n')[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aed_v9VaZGus"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "\n",
        "# Add data to a pandas dataframe\n",
        "df = pd.DataFrame(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoSkN5G5awFn",
        "outputId": "2495272a-db89-4e51-b817-60d98ab22e6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98939"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "(df['text'].values == '').sum() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tirA2DGedjoT"
      },
      "source": [
        "Play with the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiyirBGJbM_0",
        "outputId": "1e73ddd6-e28e-4208-d063-59fb704c59d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries and download example data\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#defining the object for stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.decomposition import PCA\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaCfj4gAbskV"
      },
      "outputs": [],
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.append('of')\n",
        "\n",
        "#defining the function to remove punctuation except dot (.)\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation or i == '.'])\n",
        "    return punctuationfree\n",
        "\n",
        "def tokenization(text):\n",
        "    tokens = re.split('W+',text)\n",
        "    return tokens\n",
        "\n",
        "#defining the function to remove stopwords from tokenized text\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrVFIFAFrFMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade70fe9-48ce-460f-c3c6-d7008954b733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Keep only rows with non empty text\n",
        "df_new = df[df.text != '']\n",
        "df_new['text_preprocessed'] = df_new['text']\n",
        "\n",
        "# Reindexing\n",
        "df_new.index = range(len(df_new))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3L7TEukci38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12eb88e-85eb-4f45-a182-aafc9e8439c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Preprocessing of text\n",
        "df_new['text_preprocessed']= df_new['text_preprocessed'].apply(lambda x: remove_punctuation(x))\n",
        "nltk.download('punkt')\n",
        "df_new['text_preprocessed']= df_new['text_preprocessed'].apply(lambda x: word_tokenize(str(x)))\n",
        "df_new['text_preprocessed']= df_new['text_preprocessed'].apply(lambda x: remove_stopwords(x))\n",
        "df_new['text_preprocessed'] = [' '.join(map(str, l)) for l in df_new['text_preprocessed']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "m9K7HpN1LHSG",
        "outputId": "b7eceeea-1f8b-4d92-91ce-27e43961bf2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Clepardia Kraków KS Clepardia Kraków Polish football club based Prądnik Biały district Kraków . They currently play IV Liga fifth tier Polish football league . History . The Krowodrza – Modrzejówka Sports Society founded 1909 one earliest clubs Krakow . Initially ground located Lubelska Street next military hospital later premises current military unit Wrocławska Street . The Prądnicki Sports Club established 1930 KS Prądnik . Its operation interrupted World War II reactivated 1955 . Since 1956 clubs football ground Prądnicka Street . In 1967 merger two clubs InterFactory District Sports Club Clepardia established . Over years club sport sections football handball judo . Currently football sections age levels judo functioning . Squad . Trener'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "df_new.index = range(len(df_new))\n",
        "df_new.text_preprocessed[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8XAla361817",
        "outputId": "90861c1c-1aaa-4760-812d-e3d42aa2d18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are in  1 out of  26\n",
            "We are in  2 out of  26\n",
            "We are in  3 out of  26\n",
            "We are in  4 out of  26\n",
            "We are in  5 out of  26\n",
            "We are in  6 out of  26\n",
            "We are in  7 out of  26\n",
            "We are in  8 out of  26\n",
            "We are in  9 out of  26\n",
            "We are in  10 out of  26\n",
            "We are in  11 out of  26\n",
            "We are in  12 out of  26\n",
            "We are in  13 out of  26\n",
            "We are in  14 out of  26\n",
            "We are in  15 out of  26\n",
            "We are in  16 out of  26\n",
            "We are in  17 out of  26\n",
            "We are in  18 out of  26\n",
            "We are in  19 out of  26\n",
            "We are in  20 out of  26\n",
            "We are in  21 out of  26\n",
            "We are in  22 out of  26\n",
            "We are in  23 out of  26\n",
            "We are in  24 out of  26\n",
            "We are in  25 out of  26\n",
            "We are in  26 out of  26\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from itertools import *\n",
        "\n",
        "# Join text per 2000 document\n",
        "texts = []\n",
        "for i in range(0, len(df_new),2000):\n",
        "\ttexts.append(\" \".join(text for text in df_new.text_preprocessed[i:i+2000]))\n",
        " \n",
        "spacy.require_gpu()\n",
        "total_entities = []\n",
        "total_sentences = []\n",
        "counter = 0\n",
        "keep_sentences = []\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.max_length = 100000000\n",
        "for textt in texts:\n",
        "  counter +=1\n",
        "  print(\"We are in \", counter, \"out of \", len(texts))\n",
        "  doc = nlp(textt)\n",
        "\n",
        "  # Extract all entity labels from a given doc and add them in key value pairs ie \"PERSON\": [Barack Obama, Michelle Obama]\n",
        "  doc_entities = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
        "  # Keep also the sentences of the doc in the dict\n",
        "  doc_entities[\"sentences\"] = doc.sents\n",
        "  total_entities.append(doc_entities)\n",
        "\n",
        "# Uncommenting the below takes a lot of time but we have the sentences we need\n",
        "\n",
        "  # for sent in doc.sents:\n",
        "  #   count_people = 0\n",
        "  #   sentence_entities = []\n",
        "  #   entities = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(sent.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
        "  #   # total_entities.append(entities)\n",
        "  #   # total_sentences.append(sent)\n",
        "  #   if 'PERSON' in entities:\n",
        "  #     if len(entities['PERSON']) > 1:\n",
        "  #       keep_sentences.append(sent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sed-TcfOActm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844583bf-e697-4631-d0c0-149937f71519"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are in  1 out of 26\n",
            "We are in  2 out of 26\n",
            "We are in  3 out of 26\n",
            "We are in  4 out of 26\n"
          ]
        }
      ],
      "source": [
        "# Takes a lot of time to get the sentences that we need\n",
        "keep_sentences = []\n",
        "total_count = 0\n",
        "for entity in total_entities:\n",
        "  total_count += 1\n",
        "  print(\"We are in \",total_count, \"out of\", len(total_entities)) \n",
        "  for sentence in entity['sentences']:\n",
        "    people_in_sentence = sum(person in str(sentence) for person in entity['PERSON'])\n",
        "    if people_in_sentence > 1:\n",
        "      keep_sentences.append(str(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCtpYEMvtVs9"
      },
      "outputs": [],
      "source": [
        "test = str(keep_sentences[34])\n",
        "# test = remove_punctuation(test)\n",
        "# test = word_tokenize(str(test))\n",
        "# test = remove_stopwords(test)\n",
        "# test = ' '.join(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3R8jCUjnRn-"
      },
      "outputs": [],
      "source": [
        "displacy.render(nlp(str(test)), jupyter=True, style='ent')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDAQ_suQTa7l"
      },
      "outputs": [],
      "source": [
        "displacy.render(nlp(str(keep_sentences[48])), jupyter=True, style='ent')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6wl1dpPVBHj"
      },
      "outputs": [],
      "source": [
        "# caption_doc = nlp(textt)\n",
        "\n",
        "# print('=== Parts of speech ===')\n",
        "# for token in caption_doc:\n",
        "#     print(token.text, token.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14JE7wfpq0XE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsYN2U8ElW7E"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large',device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9uVl97vq2MR"
      },
      "outputs": [],
      "source": [
        "print(keep_sentences[152])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RswrX8dFleBx"
      },
      "outputs": [],
      "source": [
        "sentences = keep_sentences\n",
        "# Function to parse the generated text and extract the triplets\n",
        "def extract_triplets(text):\n",
        "    triplets = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "    return triplets\n",
        "for sentence in sentences:  \n",
        "\n",
        "  sentence = str(sentence)\n",
        "  # We need to use the tokenizer manually since we need special tokens.\n",
        "  extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(sentence,\n",
        "                                                                              return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
        "  extracted_triplets = extract_triplets(extracted_text[0])\n",
        "  print(extracted_triplets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tRUxd1QgAaj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "1hPIrL_qcriA4e4S92Env6Ft70AVXH5uN",
      "authorship_tag": "ABX9TyMu+rl31q28cFxoeT8DmzfN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}