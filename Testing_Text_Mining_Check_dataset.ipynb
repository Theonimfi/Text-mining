{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Theonimfi/Text-mining/blob/main/Testing_Text_Mining_Check_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "58rijbWU8BBh"
   },
   "outputs": [],
   "source": [
    "# !tar -xvf  '/content/drive/MyDrive/Data Science and AI/Text mining/enwiki20220701-stripped.tgz' -C '/content/drive/MyDrive/Data Science and AI/Text mining/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do not uncomment the below \n",
    "\n",
    "# ! pip install pynvml\n",
    "# ! git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "# ! python rapidsai-csp-utils/colab/env-check.py\n",
    "# ! bash rapidsai-csp-utils/colab/update_gcc.sh\n",
    "# import os\n",
    "# os._exit(00)\n",
    "# import condacolab\n",
    "# condacolab.install()\n",
    "# import condacolab\n",
    "# condacolab.check()\n",
    "#!python rapidsai-csp-utils/colab/install_rapids.py stable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get the data from My drive\n",
    "file1 = open('/content/drive/MyDrive/Data Science and AI/Text mining_Shared/enwiki20220701-stripped/AB/wiki_63', 'r')\n",
    "Lines = file1.readlines()\n",
    "  \n",
    "content = []\n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    content.append(json.loads(line.split('\\n')[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Add data to a pandas dataframe\n",
    "df = pd.DataFrame(content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(df['text'].values == '').sum() "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Play with the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import libraries and download example data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('of')\n",
    "\n",
    "#defining the function to remove punctuation except dot (.)\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation or i == '.'])\n",
    "    return punctuationfree\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = re.split('W+',text)\n",
    "    return tokens\n",
    "\n",
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keep only rows with non empty text\n",
    "df_new = df[df.text != '']\n",
    "df_new['text_preprocessed'] = df_new['text']\n",
    "\n",
    "# Reindexing\n",
    "df_new.index = range(len(df_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preprocessing of text\n",
    "df_new['text_preprocessed']= df_new['text_preprocessed'].apply(lambda x: remove_punctuation(x))\n",
    "nltk.download('punkt')\n",
    "df_new['text_preprocessed']= df_new['text_preprocessed'].apply(lambda x: word_tokenize(str(x)))\n",
    "df_new['text_preprocessed']= df_new['text_preprocessed'].apply(lambda x: remove_stopwords(x))\n",
    "df_new['text_preprocessed'] = [' '.join(map(str, l)) for l in df_new['text_preprocessed']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_new.index = range(len(df_new))\n",
    "df_new.text_preprocessed[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from itertools import *\n",
    "import datetime\n",
    "\n",
    "# Calculate time spend\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "spacy.require_gpu()\n",
    "counter = 0\n",
    "keep_docs = []\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Save in a list all the documents that contain more than one person\n",
    "for textt in df_new.text_preprocessed:\n",
    "  counter +=1\n",
    "  if counter % 10000 == 0:\n",
    "    end = datetime.datetime.now()\n",
    "    print(\"We are in \", counter, \"out of \", len(df_new.text_preprocessed))\n",
    "    print(\"Took:\", (end-start), \"time\")\n",
    "  doc = nlp(textt)\n",
    "  people = []\n",
    "  # Get the people entities included in a doc \n",
    "  for ent in doc.ents:\n",
    "    if ((ent.label_ == 'PERSON') and (ent.text not in people)):\n",
    "      people.append(ent.text)\n",
    "  if len(people) > 1:\n",
    "    keep_docs.append(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(keep_docs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Old way, takes a lot of time\n",
    "# import spacy\n",
    "# from spacy import displacy\n",
    "# from itertools import *\n",
    "# import datetime\n",
    "\n",
    "# start = datetime.datetime.now()\n",
    "\n",
    "# # Join text per 2000 document\n",
    "# texts = []\n",
    "# for i in range(0, len(df_new),2000):\n",
    "# \ttexts.append(\" \".join(text for text in df_new.text_preprocessed[i:i+2000]))\n",
    " \n",
    "# spacy.require_gpu()\n",
    "# total_entities = []\n",
    "# counter = 0\n",
    "# # keep_sentences = []\n",
    "# keep_docs = []\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# nlp.max_length = 100000000\n",
    "# people = []\n",
    "# for textt in df_new.text_preprocessed:\n",
    "#   counter +=1\n",
    "#   if counter % 10000 == 0:\n",
    "#     end = datetime.datetime.now()\n",
    "#     print(\"We are in \", counter, \"out of \", len(df_new.text_preprocessed))\n",
    "#     print(\"Took:\", (end-start), \"time\")\n",
    "#   doc = nlp(textt)\n",
    "#   for sent in doc.sents:\n",
    "#     count_people = 0\n",
    "#     sentence_entities = []\n",
    "#     # Extract all entity labels from a given doc and add them in key value pairs ie \"PERSON\": [Barack Obama, Michelle Obama]\n",
    "#     entities = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(sent.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
    "#     if 'PERSON' in entities:\n",
    "#       if len(entities['PERSON']) > 1:\n",
    "#         keep_sentences.append(sent)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Second option, if we save all the doc_entities in a list (takes also a lot of time)\n",
    "# keep_sentences = []\n",
    "# total_count = 0\n",
    "# for entity in total_entities:\n",
    "#   total_count += 1\n",
    "#   print(\"We are in \",total_count, \"out of\", len(total_entities)) \n",
    "#   for sentence in entity['sentences']:\n",
    "#     people_in_sentence = sum(person in str(sentence) for person in entity['PERSON'])\n",
    "#     if people_in_sentence > 1:\n",
    "#       keep_sentences.append(str(sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for doc in keep_docs:\n",
    "  for sent in doc.sents:\n",
    "    test = str(sent)\n",
    "    break;\n",
    "  break;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(test)), jupyter=True, style='ent')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14JE7wfpq0XE"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsYN2U8ElW7E"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large',device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9uVl97vq2MR"
   },
   "outputs": [],
   "source": [
    "print(keep_sentences[152])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RswrX8dFleBx"
   },
   "outputs": [],
   "source": [
    "sentences = keep_sentences\n",
    "# Function to parse the generated text and extract the triplets\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "for sentence in sentences:  \n",
    "\n",
    "  sentence = str(sentence)\n",
    "  # We need to use the tokenizer manually since we need special tokens.\n",
    "  extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(sentence,\n",
    "                                                                              return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "  extracted_triplets = extract_triplets(extracted_text[0])\n",
    "  print(extracted_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tRUxd1QgAaj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNEkrlM7yNpYlnWEExbSTh2",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1hPIrL_qcriA4e4S92Env6Ft70AVXH5uN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}